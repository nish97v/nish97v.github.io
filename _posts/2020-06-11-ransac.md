---
title: 'RANSAC'
date: 2020-6-11
permalink: /posts/2020/11/ransac/
tags:
  - computer vision
  - machine learning
  - model fitting
---

Looking into why RANSAC is used and how to perform robust model fitting using RANSAC

# Robust estimation using RANSAC 

### Why RANSAC
Feature matching to compute transformation from one image to another can be considered as a model fitting problem and therefore we can use model fitting techniques like least squares and total least squares to fit the model.  However, the problem with such techniques is that they are very sensitive to outliers. This leads to large change in model parameters due to a small amount of outliers.
One way to tackle this problem is with the use of robust estimators. These estimators use a function that takes the error (in distance) and an extrinsic scale parameter $\sigma$ as input and gives the error measure that the model tries to minimize.   
The problem with these robust estimators is that they are very sensitive to the scale and selecting the right scale can be a hassle. Therefore, we need a technique for model fitting that is robust to noisy data and also not very dependent on a scale parameter. This is where RANSAC (Random Sample Consensus) comes into the picture.

The best matches might not all be correct. Some of them might be random.

### Main Idea
1. Randomly pick some points to define the model.
2. Repeat until we find the best model with the most inliers.

Inliers are points which actually fit the model (points within a threshold distance from the model)

### General RANSAC Algorithm
The general RANSAC algorithm can be given as:
- Randomly select $s$ points to form a sample
- Fit the model to those $s$ points
- Find the consensus set $C_i$  which is the set of points (from all the points) that are within error bounds ($t$) of the model
- Terminate and return the model if $$|C_i|>T$$
- Repeat for $N$ trials and return the model with $max |C_i|$

### Choosing the parameters
1. **Initial number of points in the minimal set $s$** <br/>
Minimum number of points needed to fit the model is mostly used as $s$ <br/>
Line fitting : $s = 2$ <br/>
Homography: $s = 4$
2. **Distance threshold $t$** <br/>
    Add the distance image <br/>
	- We choose a $t$ value such that the probability for inlier is high
	- We assume zero-mean Gaussian noise with standard deviation 
	
	The noise in the data is assumed to be sampled from a Gaussian distribution with variance $\sigma^2$. Then the distance $d$ (from the point to the model line) has Chi distribution $(\chi^2_k)$ with $k$ degrees of freedom where $k$ is the dimension of the Gaussian. This is the case because the distance $d$ is given as the sum of squared errors. 

	![Chi-square pdf.svg](https://upload.wikimedia.org/wikipedia/commons/thumb/3/35/Chi-square_pdf.svg/1920px-Chi-square_pdf.svg.png)
	For a 1-D Gaussian, if we set the cumulative threshold to be $95\%$ (such that $95\%$ of the inliers would fall within that threshold), then $t^2 = 3.84 \sigma^2$. This means that if the distance $d$ is less than $t$, we consider that point as an inlier.
	
3. **Number of samples $N$** <br/>
	This is the number of times we will draw $s$ points randomly. This is basically the number of models we will try and check. 
	- We choose $N$ such that with probability $p$ at least one random sample set $C_i$ is free from outliers. 
	- We choose $N$ based on the outlier ratio $e$.

	Since we are randomly picking points, we have to fix the probability that we can get a good model. Therefore, $p$ is the probability that at least one of the samples will have all the points from the correct model.
	We will also look into the percentage of bad points (outliers) to fix the number of samples.  

### How to calculate $N$ 
We have the following parameters to calculate $N$
- $s$ : number of points to fit model
- $p$ : probability of success
- $e$ : outlier ratio (proportion of outliers)
- $1-e$ : inlier ratio (proportion of inliers)

The probability that a sample set has points that are all inliers is: <br/>
<center>$$ P(\text{sample set with all inliers}) = (1-e)^s $$</center>  <br/>
This is the case since the points are independent of each other and the number of points in the sample set is $s$.
The probability that a sample set has at least one outlier is: <br/>
<center>$$  P(\text{sample set with at least one outlier}) = (1-(1-e)^s) $$ </center> <br/>
This is given from the fact that $P(\text{at least one of }\overline{A}) = 1 - P(\text{all of }A)$. <br/>
Since the samples are independent, the probability that all N samples have at least one outlier is: <br/>
<center>$$ P(\text{all $N$ samples have outliers}) = (1-(1-e)^s)^N $$</center> <br/> 
The probability that at least one sample does not have outliers is: <br/>
<center>$$ P(\text{at least one sample does not have outliers}) = (1-(1-(1-e)^s)^N) $$ </center><br/>
We already know that we want the probability that at least one sample does not have outliers to be equal to $p$. <br/>
<center>$$ P(\text{at least one sample does not have outliers}) = p $$ </center> <br/>
Substituting the probability, we get: <br/>
<center>$$ (1-(1-(1-e)^s)^N) = p $$</center> <br/>
Therefore, by solving for $N$, we get the following equation: <br/>
<center>$$ N = log(1-p)/(log(1-(1-e)^s) $$</center>

![alt text](https://github.com/nish97v/nish97v.github.io/blob/master/images/ransac/ransac-N.gif "How to find N")

### Looking into $N$

Given below is a table of the N values for corresponding sample size $s$ and proportion of outliers $e$ for $p = 0.99$

| $s$ 	| $5\%$ 	| $10\%$ 	| $20\%$ 	| $25\%$ 	| $30\%$ 	| $40\%$ 	| $50\%$ 	|
|---	|----	|-----	|-----	|-----	|-----	|-----	|------	|
| $2$ 	| $2$  	| $3$   	| $5$   	| $6$   	| $7$   	| $11$  	| $17$   	|
| $4$ 	| $3$  	| $5$   	| $9$   	| $13$  	| $17$  	| $34$  	| $72$   	|
| $8$ 	| $5$  	| $9$   	| $26$  	| $44$  	| $78$  	| $272$ 	| $1177$ 	|

We can see that even for an outlier ratio of $50\%$ in line fitting with $s=2$ with $p=2$, we only need to  perform $17$ iterations. 
Similarly for computing homography, even with $50\%$ outlier ratio with $72$ random samples, we have $99\%$ chance that there exist at least one sample with all inliers.
We also see that $N$ grows steeply with the number of points per sample $s$. <br/> 
**The main point that we need to note about $N$ is that it does not depend on the total number of points!**
This makes the algorithm size-invariant to the number of points in the data. This is one of the main reasons why RANSAC is so widely used.

### Adaptive procedure
One problem with the general RANSAC algorithm is that it's hard to know the outlier ratio a priori. We can pick for the worst case and select $e$ to be equal to a high percentage and update it's value based of the number of inliers found in each iteration.
- Set $N=\infty$, $\text{count}=0$ and $e=1.0$
- While $N > \text{count}:$
	- Pick a random sample and compute the number of inliers
	- Set $e_{update} = 1 - \frac{\text{number of inliers}}{\text{total number of points}}$
	- If $e_{update} < e :$
		- Set $e=e_{update}$
		- Recompute $N = log(1-p)/(log(1-(1-e)^s)$
	- Increment $\text{count}$
	
### Implementation details
In the case of matching features, once we've found a sample which has the maximum number of points in the consensus set $|C_i|$, we fit the model using the average of all the inlier vectors. This basically means that for the final model that fits the data, we are not only using the sample that led to the $max |C_i|$ but also all the points that we're "inliers" to its model. This also helps to average out the Gaussian noise of all the vectors (including those present in the best sample)

### Advantages
- Simple algorithm
- Applicable to many problems and works well in practice
- Robust to large number of outliers
- Does not depend on the number of points

### Disadvantages
- Computation time increases quickly with the number of parameters
- Does not work well for approximate models (when the model doesn't properly match the underlying structure of the data)

### Applications
- Computing the homography (or other image transformations)
- Estimating the fundamental matrix
- Robotic vision

### Picture of performance of RANSAC vs Least Squares




``` python
```


Reference : [Introduction to Computer Vision - Udacity](https://classroom.udacity.com/courses/ud810)

 